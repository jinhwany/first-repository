{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663ff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f0b4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('data/ratings_train.txt')\n",
    "test_data = pd.read_table('data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23a07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "  train_data.drop_duplicates(subset=['document'], inplace=True)  # 데이터 중복 제거\n",
    "  train_data = train_data.dropna(how = 'any')  # NaN 결측치 제거\n",
    "  test_data.drop_duplicates(subset=['document'], inplace=True)  # 데이터 중복 제거\n",
    "  test_data = test_data.dropna(how = 'any')  # NaN 결측치 제거\n",
    "  \n",
    "  X_train = []  # 학습용 데이터\n",
    "  for sentence in train_data['document']:\n",
    "      temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "      temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "      X_train.append(temp_X)  # 단어 저장\n",
    "\n",
    "  X_test = []  # 테스트용 데이터\n",
    "  for sentence in test_data['document']:\n",
    "      temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "      temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "      X_test.append(temp_X)  # 단어 저장\n",
    "  \n",
    "  words = np.concatenate(X_train).tolist()  # array 합치고 list로 변환\n",
    "  counter = Counter(words)  # list의 요소 개수 세기 => 딕셔너리 {단어: 개수} 형태로 반환\n",
    "  counter = counter.most_common(10000-4)  # 최빈값 (10000-4)개 => 가장 마지막 4개를 제외한 나머지\n",
    "  vocab = ['', '', '', ''] + [key for key, _ in counter]  # 단어 저장 list # , , , \n",
    "  word_to_index = {word:index for index, word in enumerate(vocab)}  # 딕셔너리 {단어: 인덱스} 형태\n",
    "  \n",
    "  # 중첩 함수\n",
    "  def wordlist_to_indexlist(wordlist):\n",
    "      return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "      \n",
    "  # list(map(함수, 리스트)): 리스트의 모든 요소를 지정된 함수로 처리한 결과를 리스트로 만듦\n",
    "  X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "  X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "      \n",
    "  return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22075271",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6abcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['']]+[word_to_index[word] if word in word_to_index else word_to_index[''] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '' for index in encoded_sentence[1:])  #[1:]를 통해 를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85439bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 74, 919, 4, 4, 39, 228, 20, 33, 748]\n",
      "라벨:  0\n",
      "1번째 리뷰 문장 길이:  10\n",
      "2번째 리뷰 문장 길이:  17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "\n",
    "print('1번째 리뷰 문장 길이: ', len(X_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(X_train[1]))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e825e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a08bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n",
      "(49157, 41)\n"
     ]
    }
   ],
   "source": [
    "# 짧은 문장 앞(pre)에 패딩 추가. post보다 pre가 효율이 더 좋음!\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"\"],\n",
    "                                                        padding='pre', # 혹은 'post'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"\"],\n",
    "                                                       padding='pre', # 혹은 'post'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)  # (146182, 41)\n",
    "print(X_test.shape)  # (49157, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b301f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(word_to_index)  # 어휘 사전의 크기: 10000개 단어\n",
    "word_vector_dim = 200   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. (변경 가능한 하이퍼 파라미터)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c9d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         1999400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 6688      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,006,169\n",
      "Trainable params: 2,006,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 1: RNN \n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model_lstm.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_lstm.summary()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c778bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         1999400   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          22416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,023,769\n",
      "Trainable params: 2,023,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 2: 1-D CNN\n",
    "\n",
    "model_1d_cnn = tf.keras.Sequential()\n",
    "model_1d_cnn.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_1d_cnn.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_1d_cnn.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model_1d_cnn.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model_1d_cnn.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model_1d_cnn.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model_1d_cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_1d_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3dcb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         1999400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 1608      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,001,017\n",
      "Trainable params: 2,001,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 3: GlobalMaxPooling1D() 레이어 1개만 사용\n",
    "\n",
    "model_gmp1d = tf.keras.Sequential()\n",
    "model_gmp1d.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_gmp1d.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model_gmp1d.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model_gmp1d.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_gmp1d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35cc1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29236, 41)\n",
      "(29236,)\n",
      "(116946, 41)\n",
      "(116946,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train : Validation : Test 을 일반적으로 6 : 2 : 2로 이용\n",
    "\n",
    "# 일반적으로 전체 데이터 중 80%를 학습으로, 20%를 검증으로 사용하는 것이 좋다고 한다.\n",
    "# validation set (총 데이터 개수 * 0.2)건 분리\n",
    "x_val_len = int(len(X_train)*0.2)\n",
    "y_val_len = int(len(y_train)*0.2)\n",
    "\n",
    "x_val = X_train[:x_val_len]   \n",
    "y_val = y_train[:y_val_len]\n",
    "\n",
    "print(x_val.shape)  # (29236, 41)\n",
    "print(y_val.shape)  # (29236,)\n",
    "\n",
    "# validation set을 제외한 나머지 (총 데이터 개수 * 0.8)건\n",
    "partial_x_train = X_train[x_val_len:]  \n",
    "partial_y_train = y_train[y_val_len:]\n",
    "\n",
    "print(partial_x_train.shape)  # (116946, 41)\n",
    "print(partial_y_train.shape)  # (116946,)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642affe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# monitor='val_loss': validation set 의 loss 를 monitoring 한다.\n",
    "# mode='min': loss 의 경우, performance measure가 최소화 시키는 방향으로 training 이 진행되므로 min 을 지정한다. => keras에서 알아서 적절한 epoch에서 training을 멈춘다.\n",
    "# verbose=1: 언제 keras 에서 training 을 멈추었는지를 화면에 출력할 수 있다.\n",
    "# patience=5: patience 는 성능이 증가하지 않는 epoch 을 몇 번이나 허용할 것인가를 주관적 기준으로 정의한다. 성능이 증가하지 않는다고 바로 멈추는 것은 효과적이지 않을 수 있기 때문이다. 사용한 데이터와 모델의 설계에 따라 최적의 값이 바뀔 수 있다. \n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f0f8f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "229/229 [==============================] - 3s 8ms/step - loss: 0.1447 - accuracy: 0.9458 - val_loss: 0.4711 - val_accuracy: 0.8479\n",
      "Epoch 2/5\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 0.1295 - accuracy: 0.9526 - val_loss: 0.4949 - val_accuracy: 0.8446\n",
      "Epoch 3/5\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 0.1194 - accuracy: 0.9568 - val_loss: 0.5408 - val_accuracy: 0.8438\n",
      "Epoch 4/5\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 0.1113 - accuracy: 0.9606 - val_loss: 0.5564 - val_accuracy: 0.8405\n",
      "Epoch 5/5\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 0.1047 - accuracy: 0.9638 - val_loss: 0.5836 - val_accuracy: 0.8399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_lstm.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_lstm = model_lstm.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ceb5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.4525 - accuracy: 0.8424\n",
      "[0.45247581601142883, 0.842382550239563]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "results_lstm = model_lstm.evaluate(X_test,  y_test, verbose=2)  # (loss, accuracy)\n",
    "\n",
    "print(results_lstm)  # (0.3667, 0.8479)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd74535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "229/229 [==============================] - 15s 8ms/step - loss: 0.4306 - accuracy: 0.8000 - val_loss: 0.3387 - val_accuracy: 0.8522\n",
      "Epoch 2/5\n",
      "229/229 [==============================] - 1s 7ms/step - loss: 0.2964 - accuracy: 0.8761 - val_loss: 0.3256 - val_accuracy: 0.8601\n",
      "Epoch 3/5\n",
      "229/229 [==============================] - 1s 7ms/step - loss: 0.2291 - accuracy: 0.9112 - val_loss: 0.3428 - val_accuracy: 0.8578\n",
      "Epoch 4/5\n",
      "229/229 [==============================] - 2s 7ms/step - loss: 0.1553 - accuracy: 0.9441 - val_loss: 0.3899 - val_accuracy: 0.8526\n",
      "Epoch 5/5\n",
      "229/229 [==============================] - 1s 6ms/step - loss: 0.0949 - accuracy: 0.9683 - val_loss: 0.4635 - val_accuracy: 0.8491\n"
     ]
    }
   ],
   "source": [
    "model_1d_cnn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_1d_cnn = model_1d_cnn.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c53185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.4873 - accuracy: 0.8424\n",
      "[0.48730096220970154, 0.8424232602119446]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 평가\n",
    "results_1d_cnn = model_1d_cnn.evaluate(X_test,  y_test, verbose=2)  # (loss, accuracy)\n",
    "\n",
    "print(results_1d_cnn)  # (0.4554, 0.8424)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1650e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "229/229 [==============================] - 2s 5ms/step - loss: 0.4750 - accuracy: 0.7971 - val_loss: 0.3474 - val_accuracy: 0.8485\n",
      "Epoch 2/5\n",
      "229/229 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8675 - val_loss: 0.3339 - val_accuracy: 0.8543\n",
      "Epoch 3/5\n",
      "229/229 [==============================] - 1s 4ms/step - loss: 0.2670 - accuracy: 0.8916 - val_loss: 0.3389 - val_accuracy: 0.8562\n",
      "Epoch 4/5\n",
      "229/229 [==============================] - 1s 4ms/step - loss: 0.2269 - accuracy: 0.9109 - val_loss: 0.3536 - val_accuracy: 0.8563\n",
      "Epoch 5/5\n",
      "229/229 [==============================] - 1s 5ms/step - loss: 0.1871 - accuracy: 0.9305 - val_loss: 0.3772 - val_accuracy: 0.8544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_gmp1d.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_gmp1d = model_gmp1d.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c962dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.3893 - accuracy: 0.8476\n",
      "[0.38934779167175293, 0.8475903868675232]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 평가\n",
    "results_gmp1d = model_gmp1d.evaluate(X_test,  y_test, verbose=2)  # (loss, accuracy)\n",
    "\n",
    "print(results_gmp1d)  # (0.3834, 0.8461)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd320f",
   "metadata": {},
   "source": [
    "출처 : https://github.com/HRPzz/AIFFEL/blob/main/EXPLORATION/Node_06/%5BE-06%5D%20Naver_movie_sentiment_analysis.ipynb\n",
    "\n",
    "과제를 보고서 양이 너무 많아서 손도 못대다가, 루브릭 기준으로 하나라도 충족시켜보자는 생각에 무작정 찾아서 그대로 복붙을 하면서 대충 어떤식으로 진행되는지 흐름이라도 파악해보려고 했다.\n",
    "\n",
    "모델들을 각자 가져와서 똑같이 훈련을 해도 결과값이 다 유의미하게 달랐고, 특히 train test 셋을 8:2로 기존에 알고있었는데, cs231n 강의에서 배운 validation 과정을 코드에 적용시켜 결과적으론 8:2지만 (6:2):2 로 결과값을 확인하는게 참신했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
